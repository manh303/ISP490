# Spark SQL Settings - OPTIMIZED FOR BIG DATA
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionSize=64MB
spark.sql.adaptive.coalescePartitions.initialPartitionNum=200
spark.sql.adaptive.advisoryPartitionSizeInBytes=128MB
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
spark.sql.adaptive.skewJoin.enabled=true

# Memory Settings - INCREASED FOR HIGH THROUGHPUT
spark.executor.memory=4g
spark.driver.memory=2g
spark.executor.cores=4
spark.driver.cores=2
spark.driver.maxResultSize=2g
spark.executor.memoryFraction=0.8
spark.executor.memoryStorageLevel=MEMORY_AND_DISK_SER
spark.sql.execution.arrow.maxRecordsPerBatch=10000

# Dynamic Allocation - ENHANCED FOR SCALING
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=8
spark.dynamicAllocation.initialExecutors=2
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.schedulerBacklogTimeout=1s

# Streaming Settings - HIGH PERFORMANCE TUNING
spark.streaming.backpressure.enabled=true
spark.streaming.kafka.maxRatePerPartition=10000
spark.sql.streaming.metricsEnabled=true
spark.sql.streaming.stateStore.maintenanceInterval=30s
spark.sql.streaming.minBatchesToRetain=10
spark.sql.streaming.maxBatchesToRetainInMemory=5

# Kafka Settings
spark.sql.kafka.bootstrap.servers=kafka:29092

# Checkpoint Directory
spark.sql.streaming.checkpointLocation=/app/checkpoints

# Event Log
spark.eventLog.enabled=true
spark.eventLog.dir=/app/logs

# History Server
spark.history.fs.logDirectory=/app/logs

# Classpath for external JARs
spark.driver.extraClassPath=/opt/bitnami/spark/jars-ext/*
spark.executor.extraClassPath=/opt/bitnami/spark/jars-ext/*

# Network settings - OPTIMIZED
spark.network.timeout=800s
spark.sql.execution.arrow.pyspark.enabled=true
spark.rpc.askTimeout=600s
spark.rpc.lookupTimeout=600s

# Shuffle Settings - HIGH PERFORMANCE
spark.sql.shuffle.partitions=400
spark.shuffle.service.enabled=true
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.shuffle.file.buffer=64k
spark.shuffle.io.maxRetries=5
spark.shuffle.io.retryWait=5s

# IO Settings - ENHANCED THROUGHPUT
spark.serializer.objectStreamReset=100
spark.sql.files.maxPartitionBytes=268435456
spark.sql.broadcastTimeout=600s
spark.sql.execution.arrow.maxRecordsPerBatch=20000

# Catalyst Optimizer Settings
spark.sql.cbo.enabled=true
spark.sql.cbo.joinReorder.enabled=true
spark.sql.statistics.histogram.enabled=true

# Garbage Collection Tuning
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1PrintGCDetails -XX:OnOutOfMemoryError='kill -9 %p' -XX:G1HeapRegionSize=16m
spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1PrintGCDetails -XX:OnOutOfMemoryError='kill -9 %p'

# Compression Settings
spark.io.compression.codec=snappy
spark.sql.parquet.compression.codec=snappy
spark.sql.orc.compression.codec=snappy

# Caching Settings
spark.sql.inMemoryColumnarStorage.compressed=true
spark.sql.inMemoryColumnarStorage.batchSize=20000

# Streaming Specific Optimizations
spark.sql.streaming.forceDeleteTempCheckpointLocation=true
spark.sql.streaming.schemaInference=true
spark.sql.streaming.unsupportedOperationCheck=false