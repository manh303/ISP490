#!/usr/bin/env python3
"""
Script d·ªçn d·∫πp v√† t√°i c·∫•u tr√∫c d·ª± √°n ch·ªâ t·∫≠p trung v√†o
D·ªÆ LI·ªÜU S·∫¢N PH·∫®M ƒêI·ªÜN T·ª¨ TMƒêT VI·ªÜT NAM
"""

import os
import shutil
import pandas as pd
from pathlib import Path
import json
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VietnamElectronicsDataCleaner:
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.data_dir = self.project_root / "data"
        self.backup_dir = self.project_root / "data_backup"
        self.cleaned_dir = self.data_dir / "vietnam_electronics_clean"

        # C√°c file c·∫ßn GI·ªÆ L·∫†I (ch·ªâ li√™n quan ƒë·∫øn ƒëi·ªán t·ª≠ VN)
        self.files_to_keep = [
            # D·ªØ li·ªáu ƒëi·ªán t·ª≠
            "data/electronics/electronics_*.csv",
            "data/electronics/electronics_analysis_*.json",

            # D·ªØ li·ªáu Vietnam consolidated (ch·ªâ products)
            "data/consolidated/vietnam_products_consolidated_*.csv",

            # D·ªØ li·ªáu Vietnam integrated (ch·ªâ products)
            "data/integrated/vietnamese_products_integrated_*.csv",

            # D·ªØ li·ªáu ƒëi·ªán t·ª≠ expanded
            "data/expanded_real/dummyjson_expanded_electronics.csv",
            "data/expanded_real/platzi_electronics.csv",

            # D·ªØ li·ªáu real datasets
            "data/real_datasets/dummyjson_electronics.csv",
            "data/real_datasets/kaggle_electronics_filtered.csv",
        ]

        # Th∆∞ m·ª•c c·∫ßn X√ìA HO√ÄN TO√ÄN
        self.dirs_to_remove = [
            "data/raw",
            "data/clean",
            "data/backups",
            "data/lake",
            "data/logs",
            "data/models",
            "data/mongo_backups",
            "data/warehouse",
            "data/vietnam_only"
        ]

        # File ƒë∆°n l·∫ª c·∫ßn X√ìA
        self.files_to_remove = [
            "data/customers.csv",
            "data/consolidated/vietnam_customers_consolidated_*.csv",
            "data/consolidated/vietnam_orders_consolidated_*.csv",
            "data/consolidated/vietnam_market_insights_*.json",
            "data/integrated/vietnamese_customers_integrated_*.csv",
            "data/integrated/vietnamese_orders_integrated_*.csv",
            "data/integrated/vietnamese_transactions_integrated_*.csv"
        ]

    def create_backup(self):
        """T·∫°o backup to√†n b·ªô d·ªØ li·ªáu hi·ªán t·∫°i"""
        logger.info("üõ°Ô∏è T·∫°o backup d·ªØ li·ªáu...")

        if self.backup_dir.exists():
            shutil.rmtree(self.backup_dir)

        shutil.copytree(self.data_dir, self.backup_dir)
        logger.info(f"‚úÖ Backup ho√†n t·∫•t t·∫°i: {self.backup_dir}")

    def clean_irrelevant_data(self):
        """X√≥a c√°c d·ªØ li·ªáu kh√¥ng li√™n quan ƒë·∫øn ƒëi·ªán t·ª≠ VN"""
        logger.info("üßπ B·∫Øt ƒë·∫ßu d·ªçn d·∫πp d·ªØ li·ªáu kh√¥ng li√™n quan...")

        # X√≥a th∆∞ m·ª•c
        for dir_pattern in self.dirs_to_remove:
            dir_path = self.project_root / dir_pattern
            if dir_path.exists():
                shutil.rmtree(dir_path)
                logger.info(f"üóëÔ∏è ƒê√£ x√≥a th∆∞ m·ª•c: {dir_path}")

        # X√≥a file ƒë∆°n l·∫ª
        for file_pattern in self.files_to_remove:
            import glob
            files = glob.glob(str(self.project_root / file_pattern))
            for file in files:
                if os.path.exists(file):
                    os.remove(file)
                    logger.info(f"üóëÔ∏è ƒê√£ x√≥a file: {file}")

    def restructure_vietnam_electronics_data(self):
        """T√°i c·∫•u tr√∫c d·ªØ li·ªáu ƒëi·ªán t·ª≠ Vi·ªát Nam"""
        logger.info("üîÑ T√°i c·∫•u tr√∫c d·ªØ li·ªáu ƒëi·ªán t·ª≠ Vi·ªát Nam...")

        # T·∫°o th∆∞ m·ª•c m·ªõi
        self.cleaned_dir.mkdir(exist_ok=True, parents=True)

        # Thu th·∫≠p t·∫•t c·∫£ d·ªØ li·ªáu ƒëi·ªán t·ª≠
        electronics_data = []

        # 1. T·ª´ th∆∞ m·ª•c electronics
        electronics_dir = self.data_dir / "electronics"
        if electronics_dir.exists():
            for csv_file in electronics_dir.glob("electronics_*.csv"):
                try:
                    df = pd.read_csv(csv_file)
                    df['source_file'] = csv_file.name
                    df['data_category'] = csv_file.stem.replace('electronics_', '').replace('_20251001_213710', '')
                    electronics_data.append(df)
                    logger.info(f"üìä ƒê√£ ƒë·ªçc: {csv_file.name} - {len(df)} records")
                except Exception as e:
                    logger.error(f"‚ùå L·ªói ƒë·ªçc {csv_file}: {e}")

        # 2. T·ª´ consolidated vietnam products
        consolidated_files = list(self.data_dir.glob("consolidated/vietnam_products_consolidated_*.csv"))
        for csv_file in consolidated_files:
            try:
                df = pd.read_csv(csv_file)
                # Ch·ªâ l·∫•y s·∫£n ph·∫©m ƒëi·ªán t·ª≠
                if 'category' in df.columns:
                    df = df[df['category'].str.contains('Electronics|electronics', case=False, na=False)]
                df['source_file'] = csv_file.name
                df['data_category'] = 'vietnam_consolidated'
                electronics_data.append(df)
                logger.info(f"üìä ƒê√£ ƒë·ªçc: {csv_file.name} - {len(df)} records ƒëi·ªán t·ª≠")
            except Exception as e:
                logger.error(f"‚ùå L·ªói ƒë·ªçc {csv_file}: {e}")

        # 3. T·ª´ expanded real
        expanded_files = [
            "expanded_real/dummyjson_expanded_electronics.csv",
            "expanded_real/platzi_electronics.csv"
        ]
        for file_path in expanded_files:
            full_path = self.data_dir / file_path
            if full_path.exists():
                try:
                    df = pd.read_csv(full_path)
                    df['source_file'] = full_path.name
                    df['data_category'] = full_path.stem
                    electronics_data.append(df)
                    logger.info(f"üìä ƒê√£ ƒë·ªçc: {full_path.name} - {len(df)} records")
                except Exception as e:
                    logger.error(f"‚ùå L·ªói ƒë·ªçc {full_path}: {e}")

        if electronics_data:
            # G·ªôp t·∫•t c·∫£ d·ªØ li·ªáu
            combined_df = pd.concat(electronics_data, ignore_index=True, sort=False)

            # L√†m s·∫°ch v√† chu·∫©n h√≥a
            combined_df = self._clean_electronics_data(combined_df)

            # L∆∞u file master
            output_file = self.cleaned_dir / f"vietnam_electronics_master_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            combined_df.to_csv(output_file, index=False)
            logger.info(f"üíæ ƒê√£ l∆∞u file master: {output_file} - {len(combined_df)} records")

            # T·∫°o breakdown theo category
            self._create_category_breakdown(combined_df)

            # T·∫°o summary report
            self._create_summary_report(combined_df)

            return combined_df
        else:
            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒëi·ªán t·ª≠ n√†o!")
            return None

    def _clean_electronics_data(self, df):
        """L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu ƒëi·ªán t·ª≠"""
        logger.info("üßΩ L√†m s·∫°ch d·ªØ li·ªáu ƒëi·ªán t·ª≠...")

        # Chu·∫©n h√≥a t√™n c·ªôt
        column_mapping = {
            'product_name': 'name',
            'product_id': 'id',
            'price_vnd': 'price_vnd',
            'price_usd': 'price_usd',
            'subcategory': 'subcategory',
            'main_category': 'main_category'
        }

        for old_col, new_col in column_mapping.items():
            if old_col in df.columns and new_col not in df.columns:
                df = df.rename(columns={old_col: new_col})

        # ƒê·∫£m b·∫£o c√°c c·ªôt b·∫Øt bu·ªôc
        required_cols = ['name', 'brand', 'category', 'price_vnd']
        for col in required_cols:
            if col not in df.columns:
                df[col] = 'Unknown'

        # L·ªçc ch·ªâ s·∫£n ph·∫©m ƒëi·ªán t·ª≠
        electronics_keywords = [
            'smartphone', 'phone', 'mobile', 'tablet', 'laptop', 'computer',
            'headphone', 'speaker', 'camera', 'electronics', 'audio', 'video',
            'gaming', 'smartwatch', 'earphone', 'charger', 'iphone', 'samsung',
            'xiaomi', 'oppo', 'vivo', 'huawei', 'apple'
        ]

        # L·ªçc theo t√™n s·∫£n ph·∫©m ho·∫∑c category
        mask = False
        for keyword in electronics_keywords:
            mask |= df['name'].str.contains(keyword, case=False, na=False)
            if 'category' in df.columns:
                mask |= df['category'].str.contains(keyword, case=False, na=False)
            if 'subcategory' in df.columns:
                mask |= df['subcategory'].str.contains(keyword, case=False, na=False)

        df = df[mask]

        # X√≥a duplicates
        before_count = len(df)
        df = df.drop_duplicates(subset=['name', 'brand'], keep='first')
        after_count = len(df)

        logger.info(f"‚úÖ L√†m s·∫°ch ho√†n t·∫•t: {before_count} ‚Üí {after_count} records")

        return df

    def _create_category_breakdown(self, df):
        """T·∫°o breakdown theo t·ª´ng category"""
        logger.info("üìÇ T·∫°o breakdown theo category...")

        # Breakdown theo subcategory
        if 'subcategory' in df.columns:
            for subcategory in df['subcategory'].unique():
                if pd.notna(subcategory) and subcategory != 'Unknown':
                    subset = df[df['subcategory'] == subcategory]
                    if len(subset) > 0:
                        filename = f"vietnam_electronics_{subcategory.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}.csv"
                        output_path = self.cleaned_dir / filename
                        subset.to_csv(output_path, index=False)
                        logger.info(f"üíæ {subcategory}: {len(subset)} records ‚Üí {filename}")

    def _create_summary_report(self, df):
        """T·∫°o b√°o c√°o t·ªïng h·ª£p"""
        logger.info("üìä T·∫°o b√°o c√°o t·ªïng h·ª£p...")

        summary = {
            'report_generated': datetime.now().isoformat(),
            'total_products': len(df),
            'unique_brands': df['brand'].nunique() if 'brand' in df.columns else 0,
            'categories': {},
            'price_analysis': {},
            'data_sources': {}
        }

        # Ph√¢n t√≠ch theo category
        if 'subcategory' in df.columns:
            summary['categories'] = df['subcategory'].value_counts().to_dict()

        # Ph√¢n t√≠ch gi√°
        if 'price_vnd' in df.columns:
            price_col = pd.to_numeric(df['price_vnd'], errors='coerce')
            summary['price_analysis'] = {
                'avg_price_vnd': float(price_col.mean()) if not price_col.isna().all() else 0,
                'min_price_vnd': float(price_col.min()) if not price_col.isna().all() else 0,
                'max_price_vnd': float(price_col.max()) if not price_col.isna().all() else 0
            }

        # Ph√¢n t√≠ch ngu·ªìn d·ªØ li·ªáu
        if 'data_category' in df.columns:
            summary['data_sources'] = df['data_category'].value_counts().to_dict()

        # L∆∞u b√°o c√°o
        report_file = self.cleaned_dir / f"vietnam_electronics_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"üìã B√°o c√°o t·ªïng h·ª£p: {report_file}")
        return summary

    def update_dags_for_electronics_only(self):
        """C·∫≠p nh·∫≠t c√°c DAG ch·ªâ x·ª≠ l√Ω d·ªØ li·ªáu ƒëi·ªán t·ª≠ VN"""
        logger.info("üîß C·∫≠p nh·∫≠t DAG cho d·ªØ li·ªáu ƒëi·ªán t·ª≠ VN...")

        # T·∫°o DAG m·ªõi chuy√™n bi·ªát
        dag_content = '''#!/usr/bin/env python3
"""
Vietnam Electronics E-commerce Data Pipeline
Chuy√™n x·ª≠ l√Ω d·ªØ li·ªáu s·∫£n ph·∫©m ƒëi·ªán t·ª≠ TMƒêT Vi·ªát Nam
"""

import os
import sys
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago

# Configuration cho ƒëi·ªán t·ª≠ VN
VIETNAM_ELECTRONICS_PLATFORMS = {
    'shopee': {
        'name': 'Shopee Vietnam Electronics',
        'categories': ['smartphones', 'laptops', 'tablets', 'audio', 'gaming'],
        'api_endpoint': 'https://shopee.vn/api/v4/search/search_items?keyword=electronics'
    },
    'lazada': {
        'name': 'Lazada Vietnam Electronics',
        'categories': ['mobile-phones', 'computers', 'audio', 'cameras'],
        'api_endpoint': 'https://www.lazada.vn/catalog/?q=electronics'
    },
    'tiki': {
        'name': 'Tiki Electronics',
        'categories': ['dien-thoai-smartphone', 'laptop', 'tablet', 'am-thanh'],
        'api_endpoint': 'https://tiki.vn/api/v2/products?category=electronics'
    },
    'fptshop': {
        'name': 'FPT Shop Electronics',
        'categories': ['dien-thoai', 'laptop', 'tablet', 'am-thanh'],
        'api_endpoint': 'https://fptshop.com.vn/api/product?category=electronics'
    }
}

def collect_vietnam_electronics_data(**context):
    """Thu th·∫≠p d·ªØ li·ªáu ƒëi·ªán t·ª≠ t·ª´ c√°c platform VN"""
    import pandas as pd
    from datetime import datetime

    logger.info("üáªüá≥ Thu th·∫≠p d·ªØ li·ªáu ƒëi·ªán t·ª≠ Vi·ªát Nam...")

    # Implement collection logic here
    collected_data = []

    for platform, config in VIETNAM_ELECTRONICS_PLATFORMS.items():
        logger.info(f"üì± Thu th·∫≠p t·ª´ {config['name']}...")
        # TODO: Implement actual collection logic

    return {"collected_count": len(collected_data)}

# DAG definition
dag = DAG(
    'vietnam_electronics_pipeline',
    default_args={
        'owner': 'vietnam-electronics-team',
        'depends_on_past': False,
        'start_date': days_ago(1),
        'email_on_failure': True,
        'email_on_retry': False,
        'retries': 2,
        'retry_delay': timedelta(minutes=3),
    },
    description='Pipeline chuy√™n x·ª≠ l√Ω d·ªØ li·ªáu ƒëi·ªán t·ª≠ TMƒêT Vi·ªát Nam',
    schedule_interval='0 */6 * * *',  # M·ªói 6 gi·ªù
    catchup=False,
    tags=['vietnam', 'electronics', 'ecommerce']
)

# Tasks
collect_task = PythonOperator(
    task_id='collect_vietnam_electronics',
    python_callable=collect_vietnam_electronics_data,
    dag=dag
)
'''

        # L∆∞u DAG m·ªõi
        dag_file = self.project_root / "airflow" / "dags" / "vietnam_electronics_pipeline.py"
        dag_file.parent.mkdir(parents=True, exist_ok=True)

        with open(dag_file, 'w', encoding='utf-8') as f:
            f.write(dag_content)

        logger.info(f"üìù ƒê√£ t·∫°o DAG m·ªõi: {dag_file}")

    def run_cleanup(self):
        """Ch·∫°y to√†n b·ªô qu√° tr√¨nh d·ªçn d·∫πp"""
        logger.info("üöÄ B·∫Øt ƒë·∫ßu d·ªçn d·∫πp v√† t√°i c·∫•u tr√∫c d·ª± √°n...")

        try:
            # 1. Backup
            self.create_backup()

            # 2. D·ªçn d·∫πp
            self.clean_irrelevant_data()

            # 3. T√°i c·∫•u tr√∫c
            combined_df = self.restructure_vietnam_electronics_data()

            # 4. C·∫≠p nh·∫≠t DAGs
            self.update_dags_for_electronics_only()

            logger.info("‚úÖ Ho√†n t·∫•t d·ªçn d·∫πp v√† t√°i c·∫•u tr√∫c!")

            if combined_df is not None:
                logger.info(f"""
üéØ K·∫æT QU·∫¢ CU·ªêI C√ôNG:
- T·ªïng s·∫£n ph·∫©m ƒëi·ªán t·ª≠: {len(combined_df):,}
- Brands: {combined_df['brand'].nunique() if 'brand' in combined_df.columns else 'N/A'}
- Categories: {combined_df['subcategory'].nunique() if 'subcategory' in combined_df.columns else 'N/A'}
- D·ªØ li·ªáu ƒë√£ l∆∞u t·∫°i: {self.cleaned_dir}
- Backup t·∫°i: {self.backup_dir}
                """)

            return True

        except Exception as e:
            logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh d·ªçn d·∫πp: {e}")
            return False

if __name__ == "__main__":
    cleaner = VietnamElectronicsDataCleaner()
    success = cleaner.run_cleanup()

    if success:
        print("\n" + "="*60)
        print("üéâ D·ªåN D·∫∏P HO√ÄN T·∫§T!")
        print("="*60)
        print("‚úÖ D·ª± √°n ƒë√£ ƒë∆∞·ª£c t√°i c·∫•u tr√∫c ch·ªâ t·∫≠p trung v√†o:")
        print("   üì± S·∫¢N PH·∫®M ƒêI·ªÜN T·ª¨ TMƒêT VI·ªÜT NAM")
        print("\nüìÇ D·ªØ li·ªáu s·∫°ch t·∫°i: data/vietnam_electronics_clean/")
        print("üõ°Ô∏è Backup t·∫°i: data_backup/")
        print("üîß DAG m·ªõi: airflow/dags/vietnam_electronics_pipeline.py")
        print("="*60)
    else:
        print("‚ùå C√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh d·ªçn d·∫πp!")