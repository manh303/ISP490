# ğŸš€ BIG DATA STREAMING ARCHITECTURE GUIDE

## ğŸ“Š **ARCHITECTURE OVERVIEW**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA SOURCES  â”‚    â”‚   KAFKA CLUSTER â”‚    â”‚  SPARK CLUSTER  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ CSV Files     â”‚ â”€â”€â–¶â”‚ â€¢ Topics        â”‚ â”€â”€â–¶â”‚ â€¢ Stream Proc   â”‚
â”‚ â€¢ APIs          â”‚    â”‚ â€¢ Partitions    â”‚    â”‚ â€¢ Transformationsâ”‚
â”‚ â€¢ Real-time     â”‚    â”‚ â€¢ Replication   â”‚    â”‚ â€¢ Aggregations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                       â”‚
                                  â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MONITORING    â”‚    â”‚   RAW STORAGE   â”‚    â”‚ PROCESSED DATA  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Prometheus    â”‚    â”‚ â€¢ MongoDB       â”‚    â”‚ â€¢ PostgreSQL    â”‚
â”‚ â€¢ Grafana       â”‚    â”‚ â€¢ Data Lake     â”‚    â”‚ â€¢ Data Warehouseâ”‚
â”‚ â€¢ Kafka UI      â”‚    â”‚ â€¢ Event Store   â”‚    â”‚ â€¢ Feature Store â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ **COMPONENTS**

### **Core Infrastructure**
- **Kafka Cluster**: Message streaming & event sourcing
- **Spark Cluster**: Real-time stream processing  
- **Zookeeper**: Kafka coordination
- **Redis**: Caching & session management

### **Databases**
- **PostgreSQL**: Structured/processed data
- **MongoDB**: Raw/unstructured data
- **Redis**: Cache & real-time data

### **Processing Services**
- **Stream Producer**: CSV â†’ Kafka pipeline
- **Stream Processor**: Kafka â†’ Databases pipeline
- **Airflow**: Orchestration & scheduling

### **Monitoring Stack**
- **Prometheus**: Metrics collection
- **Grafana**: Dashboards & alerts
- **Kafka UI**: Topic monitoring

## ğŸš€ **QUICK START**

### **Step 1: Prerequisites**
```bash
# Required tools
- Docker & Docker Compose
- 16GB+ RAM recommended
- 50GB+ disk space
```

### **Step 2: Environment Setup**
```bash
# 1. Clone repository
git clone <your-repo>
cd ecommerce-dss-project

# 2. Create data directories
mkdir -p data/{raw,processed,models,checkpoints}
mkdir -p streaming/{producers,processors,tests}

# 3. Copy environment file
cp .env.example .env
# Edit .env with your Kaggle credentials

# 4. Add Kaggle credentials
echo '{"username":"your_username","key":"your_api_key"}' > kaggle.json
```

### **Step 3: Launch Streaming Infrastructure**
```bash
# Start core infrastructure
docker-compose up -d zookeeper kafka mongodb postgres redis

# Wait for services (30 seconds)
sleep 30

# Start Spark cluster
docker-compose up -d spark-master spark-worker-1 spark-worker-2

# Start streaming services
docker-compose up -d stream-producer stream-processor

# Start monitoring
docker-compose up -d prometheus grafana kafka-ui

# Start applications
docker-compose up -d backend frontend nginx
```

### **Step 4: Verify Deployment**
```bash
# Run end-to-end test
docker-compose exec stream-producer python tests/test_end_to_end_pipeline.py

# Check service health
docker-compose ps
```

## ğŸ“‹ **SERVICE URLS**

| Service | URL | Purpose |
|---------|-----|---------|
| Frontend | http://localhost:3000 | Main application |
| Backend API | http://localhost:8000 | REST API |
| Kafka UI | http://localhost:8090 | Kafka monitoring |
| Spark Master UI | http://localhost:8081 | Spark cluster |
| Airflow | http://localhost:8080 | Workflow orchestration |
| Grafana | http://localhost:3001 | Metrics dashboards |
| Prometheus | http://localhost:9090 | Metrics collection |

**Default Credentials:**
- Airflow: admin/admin123
- Grafana: admin/admin123

## ğŸ”§ **CONFIGURATION**

### **Kafka Topics**
```bash
# List topics
docker-compose exec kafka kafka-topics --bootstrap-server kafka:29092 --list

# Create custom topic
docker-compose exec kafka kafka-topics --bootstrap-server kafka:29092 \
  --create --topic my_topic --partitions 3 --replication-factor 1
```

### **Spark Jobs**
```bash
# Submit custom Spark job
docker-compose exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 \
  /app/streaming/my_job.py
```

### **Database Access**
```bash
# PostgreSQL
docker-compose exec postgres psql -U dss_user -d ecommerce_dss

# MongoDB
docker-compose exec mongodb mongosh ecommerce_dss
```

## ğŸ“Š **DATA FLOW**

### **1. Data Ingestion**
```python
# CSV â†’ Kafka Producer
stream-producer reads CSV files
â†“ 
Batches data (configurable size)
â†“
Sends to Kafka topics with partitioning
```

### **2. Stream Processing**
```python
# Kafka â†’ Spark Streaming â†’ Databases
Spark reads from Kafka topics
â†“
Applies transformations & validations  
â†“
Writes raw data to MongoDB
â†“
Writes processed data to PostgreSQL
```

### **3. Real-time Analytics**
```python
# Live aggregations & metrics
Window-based aggregations (5 min windows)
â†“
Real-time dashboards
â†“
Alerts & notifications
```

## ğŸ› ï¸ **DEVELOPMENT**

### **Adding New Data Sources**
```python
# 1. Create producer in streaming/producers/
class MyDataProducer(BaseProducer):
    def produce_data(self):
        # Your data production logic
        pass

# 2. Register topic in Kafka
# 3. Update Spark processor for new data type
# 4. Add monitoring & tests
```

### **Custom Stream Processing**
```python
# streaming/processors/custom_processor.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def process_custom_stream(spark, df):
    return df.select("*").filter(col("price") > 100)
```

### **Running Tests**
```bash
# Unit tests
docker-compose exec stream-processor python -m pytest tests/unit/

# Integration tests  
docker-compose exec stream-producer python tests/test_end_to_end_pipeline.py

# Performance tests
docker-compose exec spark-master python tests/test_performance.py
```

## ğŸ“ˆ **MONITORING & ALERTING**

### **Key Metrics to Monitor**
- **Kafka**: Message throughput, lag, partition distribution
- **Spark**: Job duration, task failures, memory usage
- **Databases**: Connection pools, query performance
- **Application**: API response times, error rates

### **Grafana Dashboard Panels**
1. **Kafka Metrics**: Topics, partitions, consumer lag
2. **Spark Metrics**: Executors, stages, streaming rates
3. **Database Health**: Connections, queries, storage
4. **Application Health**: API metrics, user activity

### **Setting Up Alerts**
```bash
# Edit monitoring/alert_rules.yml
groups:
  - name: streaming
    rules:
      - alert: KafkaLagHigh
        expr: kafka_consumer_lag > 1000
        for: 5m
        annotations:
          summary: "Kafka consumer lag is high"
```

## ğŸ” **TROUBLESHOOTING**

### **Common Issues**

**Kafka Connection Failures:**
```bash
# Check if Kafka is ready
docker-compose exec kafka kafka-broker-api-versions --bootstrap-server kafka:29092

# Reset consumer group
docker-compose exec kafka kafka-consumer-groups --bootstrap-server kafka:29092 \
  --group dss_consumers --reset-offsets --to-earliest --all-topics --execute
```

**Spark Job Failures:**
```bash
# Check Spark logs
docker-compose logs spark-master spark-worker-1

# Access Spark UI for job details
# http://localhost:8081
```

**Out of Memory Errors:**
```bash
# Increase memory in docker-compose.yml
environment:
  - SPARK_WORKER_MEMORY=4g
  - SPARK_EXECUTOR_MEMORY=2g
```

**Database Connection Issues:**
```bash
# Test PostgreSQL connection
docker-compose exec postgres pg_isready -U dss_user

# Check MongoDB
docker-compose exec mongodb mongosh --eval "db.adminCommand('ping')"
```

## ğŸš€ **SCALING**

### **Horizontal Scaling**
```yaml
# Add more Spark workers
spark-worker-3:
  image: bitnami/spark:3.4
  environment:
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark-master:7077

# Add more Kafka brokers
kafka-2:
  image: confluentinc/cp-kafka:7.4.0
  environment:
    KAFKA_BROKER_ID: 2
```

### **Vertical Scaling**
```yaml
# Increase resource limits
services:
  kafka:
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
```

## ğŸ“š **ADVANCED TOPICS**

### **Schema Evolution**
- Implement Avro schemas for structured data
- Use Confluent Schema Registry
- Plan backward compatibility

### **Exactly-Once Processing**
- Configure Kafka producer idempotence
- Use Spark checkpointing
- Implement deduplication logic

### **Multi-Region Setup**
- Configure Kafka replication across regions
- Set up database read replicas
- Implement disaster recovery

## ğŸ”’ **SECURITY**

### **Authentication & Authorization**
```bash
# Enable SASL authentication in Kafka
KAFKA_SECURITY_PROTOCOL=SASL_PLAINTEXT
KAFKA_SASL_MECHANISM=PLAIN
```

### **Network Security**
```yaml
# Use Docker secrets for sensitive data
secrets:
  - kafka_keystore
  - postgres_password
```

## ğŸ“¦ **DEPLOYMENT**

### **Production Checklist**
- [ ] Set up monitoring & alerting
- [ ] Configure backup strategies
- [ ] Implement security measures
- [ ] Set resource limits
- [ ] Configure log aggregation
- [ ] Set up disaster recovery
- [ ] Performance testing
- [ ] Security scanning

### **CI/CD Pipeline**
```yaml
# .github/workflows/deploy.yml
name: Deploy Streaming Pipeline
on:
  push:
    branches: [main]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: |
          docker-compose up -d
          docker-compose exec -T stream-producer python tests/test_pipeline.py
```

## ğŸ†˜ **SUPPORT**

### **Getting Help**
- Check logs: `docker-compose logs [service_name]`
- Monitor dashboards in Grafana
- Review Kafka UI for topic health
- Check Spark UI for job status

### **Performance Tuning**
- Optimize Kafka partition count
- Tune Spark executor memory
- Configure database connection pools
- Monitor JVM garbage collection

---

## ğŸ‰ **CONGRATULATIONS!**

You now have a **production-ready Big Data streaming architecture** with:
- âœ… **Real-time data streaming** (Kafka)
- âœ… **Stream processing** (Spark)  
- âœ… **Dual storage** (MongoDB + PostgreSQL)
- âœ… **Monitoring** (Prometheus + Grafana)
- âœ… **Orchestration** (Airflow)
- âœ… **End-to-end testing**

**Your streaming pipeline can handle millions of events per day with sub-second latency!** ğŸš€