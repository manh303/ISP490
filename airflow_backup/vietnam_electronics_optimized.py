#!/usr/bin/env python3
"""
Optimized Vietnam Electronics Pipeline DAG
DAG tá»‘i Æ°u duy nháº¥t cho xá»­ lÃ½ dá»¯ liá»‡u Ä‘iá»‡n tá»­ TMÄT Viá»‡t Nam
"""

import os
import sys
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago

# Configuration
DAG_ID = "vietnam_electronics_optimized"
DESCRIPTION = "Optimized pipeline for Vietnam Electronics E-commerce data processing"

# Default args
default_args = {
    'owner': 'vietnam-electronics-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Create DAG
dag = DAG(
    DAG_ID,
    default_args=default_args,
    description=DESCRIPTION,
    schedule_interval='0 */6 * * *',  # Every 6 hours
    catchup=False,
    max_active_runs=1,
    tags=['vietnam', 'electronics', 'optimized']
)

def collect_vietnam_electronics(**context):
    """Thu tháº­p dá»¯ liá»‡u Ä‘iá»‡n tá»­ VN tá»« collector"""
    import subprocess
    import logging

    logger = logging.getLogger(__name__)
    logger.info("ğŸ‡»ğŸ‡³ Báº¯t Ä‘áº§u thu tháº­p dá»¯ liá»‡u Ä‘iá»‡n tá»­ Viá»‡t Nam...")

    try:
        # Run the collector script
        result = subprocess.run(
            ['python', '/opt/airflow/dags/../vietnam_electronics_collector.py'],
            capture_output=True,
            text=True,
            timeout=1800  # 30 minutes timeout
        )

        if result.returncode == 0:
            logger.info("âœ… Thu tháº­p thÃ nh cÃ´ng")
            logger.info(result.stdout)
            return {"status": "success", "message": "Data collected successfully"}
        else:
            logger.error("âŒ Thu tháº­p tháº¥t báº¡i")
            logger.error(result.stderr)
            raise Exception(f"Collection failed: {result.stderr}")

    except Exception as e:
        logger.error(f"âŒ Lá»—i thu tháº­p: {e}")
        raise

def process_electronics_data(**context):
    """Xá»­ lÃ½ vÃ  lÃ m sáº¡ch dá»¯ liá»‡u Ä‘iá»‡n tá»­"""
    import pandas as pd
    import logging
    from pathlib import Path

    logger = logging.getLogger(__name__)
    logger.info("ğŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ dá»¯ liá»‡u Ä‘iá»‡n tá»­...")

    try:
        # Find latest fresh data
        fresh_dir = Path("/opt/airflow/data/vietnam_electronics_fresh")
        if not fresh_dir.exists():
            raise Exception("No fresh data directory found")

        # Get latest file
        csv_files = list(fresh_dir.glob("vietnam_electronics_fresh_*.csv"))
        if not csv_files:
            raise Exception("No fresh data files found")

        latest_file = max(csv_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ğŸ“Š Processing file: {latest_file}")

        # Load and basic processing
        df = pd.read_csv(latest_file)

        # Basic cleaning
        initial_count = len(df)
        df = df.dropna(subset=['name', 'price_vnd'])
        df = df.drop_duplicates(subset=['name', 'brand'])
        final_count = len(df)

        # Save processed data
        output_file = fresh_dir / f"processed_electronics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(output_file, index=False)

        logger.info(f"âœ… Xá»­ lÃ½ hoÃ n táº¥t: {initial_count} â†’ {final_count} records")
        logger.info(f"ğŸ’¾ Saved to: {output_file}")

        return {
            "status": "success",
            "input_records": initial_count,
            "output_records": final_count,
            "output_file": str(output_file)
        }

    except Exception as e:
        logger.error(f"âŒ Lá»—i xá»­ lÃ½: {e}")
        raise

def generate_electronics_report(**context):
    """Táº¡o bÃ¡o cÃ¡o dá»¯ liá»‡u Ä‘iá»‡n tá»­"""
    import json
    import logging

    logger = logging.getLogger(__name__)
    logger.info("ğŸ“Š Táº¡o bÃ¡o cÃ¡o dá»¯ liá»‡u Ä‘iá»‡n tá»­...")

    try:
        # Get results from previous tasks
        collection_result = context['task_instance'].xcom_pull(task_ids='collect_electronics_data')
        processing_result = context['task_instance'].xcom_pull(task_ids='process_electronics_data')

        # Create report
        report = {
            'pipeline_run': context['dag_run'].run_id,
            'execution_date': context['execution_date'].isoformat(),
            'collection_status': collection_result.get('status', 'unknown') if collection_result else 'failed',
            'processing_status': processing_result.get('status', 'unknown') if processing_result else 'failed',
            'records_processed': processing_result.get('output_records', 0) if processing_result else 0,
            'data_quality_score': 85.0,  # Placeholder
            'platforms_covered': ['Tiki', 'DummyJSON', 'FakeStore', 'Synthetic'],
            'report_generated_at': datetime.now().isoformat()
        }

        logger.info(f"ğŸ“‹ Report: {json.dumps(report, indent=2)}")

        return report

    except Exception as e:
        logger.error(f"âŒ Lá»—i táº¡o bÃ¡o cÃ¡o: {e}")
        raise

# Task definitions
collect_task = PythonOperator(
    task_id='collect_electronics_data',
    python_callable=collect_vietnam_electronics,
    dag=dag
)

process_task = PythonOperator(
    task_id='process_electronics_data',
    python_callable=process_electronics_data,
    dag=dag
)

report_task = PythonOperator(
    task_id='generate_electronics_report',
    python_callable=generate_electronics_report,
    dag=dag
)

# Task dependencies
collect_task >> process_task >> report_task

# DAG documentation
dag.doc_md = """
# Vietnam Electronics Optimized Pipeline

## Má»¥c Ä‘Ã­ch
Pipeline tá»‘i Æ°u duy nháº¥t Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u sáº£n pháº©m Ä‘iá»‡n tá»­ tá»« cÃ¡c platform TMÄT Viá»‡t Nam.

## Luá»“ng xá»­ lÃ½
1. **Collect**: Thu tháº­p dá»¯ liá»‡u tá»« Tiki, APIs vÃ  synthetic data
2. **Process**: LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u
3. **Report**: Táº¡o bÃ¡o cÃ¡o cháº¥t lÆ°á»£ng dá»¯ liá»‡u

## Lá»‹ch cháº¡y
- Má»—i 6 giá»
- KhÃ´ng cháº¡y backfill
- Tá»‘i Ä‘a 1 run concurrent

## Platforms há»— trá»£
- Tiki Vietnam API
- DummyJSON Electronics
- FakeStore API
- Vietnam Synthetic Data Generator
"""
