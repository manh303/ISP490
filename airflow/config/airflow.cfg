[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow DAGs are stored
dags_folder = /opt/airflow/dags

# The folder where airflow plugins are stored
plugins_folder = /opt/airflow/plugins

# The executor class that airflow should use
executor = CeleryExecutor

# Whether to load the default connections that ship with Airflow
load_default_connections = True

# Whether to load the examples that ship with Airflow
load_examples = False

# Whether to enable pickling for xcom
enable_xcom_pickling = True

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# When not using pools, tasks are run in the "default pool"
non_pooled_task_slot_count = 128

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to serialize DAGs and persist them in DB
store_serialized_dags = True

[database]
# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@airflow-db/airflow

# The encoding for the databases
sql_alchemy_engine_encoding = utf-8

# Disable pool pre-ping to avoid issues
sql_alchemy_pool_pre_ping = True

[logging]
# The folder where airflow logs are stored
base_log_folder = /opt/airflow/logs

# Logging level
logging_level = INFO

# Log format for tasks
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[celery]
# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers
worker_concurrency = 16

# Celery broker URL
broker_url = redis://redis:6379/0

# Celery result backend
result_backend = db+postgresql://airflow:airflow@airflow-db/airflow

[webserver]
# The base url of your website
base_url = http://localhost:8080

# Default timezone to display all dates in the UI
default_ui_timezone = Asia/Ho_Chi_Minh

# The port on which to run the web server
web_server_port = 8080

# Secret key used to run your flask app
secret_key = your-secret-key-here

# Whether to expose the configuration file in the web server
expose_config = False

# Set to true to turn on authentication
authenticate = True

# Filter the list of dags by owner name
filter_by_owner = False

[api]
# How to authenticate users of the API
auth_backends = airflow.api.auth.backend.basic_auth

[scheduler]
# Task instances listen for external kill signal
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# Statsd metrics
statsd_on = False

# How often should stats be printed to the logs
print_stats_interval = 30

# How many seconds to wait between file-parsing loops to prevent the logs from being spammed
min_file_process_interval = 0

# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 300

# How often should the scheduler check for orphaned tasks and SchedulerJobs
orphaned_tasks_check_interval = 300.0

# Length of time a task instance should remain in the scheduled state when it is missing
child_process_timeout = 60

# How long before timing out a python file import
dagbag_import_timeout = 30.0

# Should the scheduler try to pickup tasks from other schedulers
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by default
catchup_by_default = False

[email]
# Configuration email backend and whether to send email alerts on retry or failure
email_backend = airflow.utils.email.send_email_smtp