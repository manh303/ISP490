#!/usr/bin/env python3
"""
Airflow DAG Optimization Report
Ph√¢n t√≠ch v√† t·ªëi ∆∞u h√≥a c√°c DAG Airflow cho d·ª± √°n ƒëi·ªán t·ª≠ VN
"""

import os
import re
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AirflowOptimizer:
    def __init__(self):
        self.airflow_dir = Path("airflow/dags")
        self.backup_dir = Path("airflow_backup")
        self.report_file = Path(f"AIRFLOW_OPTIMIZATION_REPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")

        # Ph√¢n lo·∫°i DAG theo m·ª•c ƒë√≠ch
        self.dag_categories = {
            'KEEP_ELECTRONICS': [],    # Gi·ªØ l·∫°i - li√™n quan ƒëi·ªán t·ª≠ VN
            'KEEP_CORE': [],          # Gi·ªØ l·∫°i - core functionality
            'REMOVE_DUPLICATE': [],    # X√≥a - tr√πng l·∫∑p
            'REMOVE_UNUSED': [],      # X√≥a - kh√¥ng s·ª≠ d·ª•ng
            'REMOVE_TESTING': []      # X√≥a - testing only
        }

    def analyze_dag_file(self, file_path):
        """Ph√¢n t√≠ch m·ªôt file DAG"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            file_info = {
                'file_name': file_path.name,
                'size_kb': round(file_path.stat().st_size / 1024, 2),
                'line_count': len(content.split('\n')),
                'dag_id': self._extract_dag_id(content),
                'schedule': self._extract_schedule(content),
                'description': self._extract_description(content),
                'main_functions': self._extract_functions(content),
                'dependencies': self._extract_dependencies(content),
                'electronics_related': self._is_electronics_related(content),
                'vietnam_related': self._is_vietnam_related(content),
                'complexity_score': self._calculate_complexity(content)
            }

            return file_info

        except Exception as e:
            logger.error(f"Error analyzing {file_path}: {e}")
            return None

    def _extract_dag_id(self, content):
        """Tr√≠ch xu·∫•t DAG ID"""
        patterns = [
            r"dag_id\s*=\s*['\"]([^'\"]+)['\"]",
            r"DAG\(\s*['\"]([^'\"]+)['\"]",
            r"DAG_ID\s*=\s*['\"]([^'\"]+)['\"]"
        ]

        for pattern in patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1)
        return "Unknown"

    def _extract_schedule(self, content):
        """Tr√≠ch xu·∫•t schedule interval"""
        patterns = [
            r"schedule_interval\s*=\s*['\"]([^'\"]+)['\"]",
            r"schedule\s*=\s*['\"]([^'\"]+)['\"]"
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1)
        return "Unknown"

    def _extract_description(self, content):
        """Tr√≠ch xu·∫•t m√¥ t·∫£ DAG"""
        lines = content.split('\n')
        for i, line in enumerate(lines[:20]):  # Check first 20 lines
            if '"""' in line and i < 10:
                desc_lines = []
                for j in range(i+1, min(i+10, len(lines))):
                    if '"""' in lines[j]:
                        break
                    desc_lines.append(lines[j].strip())
                return ' '.join(desc_lines)[:200]  # First 200 chars
        return "No description"

    def _extract_functions(self, content):
        """Tr√≠ch xu·∫•t c√°c function ch√≠nh"""
        functions = re.findall(r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\(', content)
        return functions[:10]  # First 10 functions

    def _extract_dependencies(self, content):
        """Tr√≠ch xu·∫•t dependencies"""
        imports = re.findall(r'from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import', content)
        imports += re.findall(r'import\s+([a-zA-Z_][a-zA-Z0-9_.]*)', content)
        return list(set(imports))[:10]  # Unique, first 10

    def _is_electronics_related(self, content):
        """Ki·ªÉm tra c√≥ li√™n quan ƒë·∫øn ƒëi·ªán t·ª≠ kh√¥ng"""
        electronics_keywords = [
            'electronics', 'smartphone', 'laptop', 'tablet', 'audio',
            'camera', 'headphone', 'gaming', 'mobile', 'computer'
        ]

        content_lower = content.lower()
        return any(keyword in content_lower for keyword in electronics_keywords)

    def _is_vietnam_related(self, content):
        """Ki·ªÉm tra c√≥ li√™n quan ƒë·∫øn Vietnam kh√¥ng"""
        vietnam_keywords = [
            'vietnam', 'vietnamese', 'vn_', 'shopee', 'lazada', 'tiki',
            'fptshop', 'cellphones', 'sendo'
        ]

        content_lower = content.lower()
        return any(keyword in content_lower for keyword in vietnam_keywords)

    def _calculate_complexity(self, content):
        """T√≠nh ƒëi·ªÉm ph·ª©c t·∫°p c·ªßa DAG"""
        score = 0

        # Function count
        function_count = len(re.findall(r'def\s+\w+', content))
        score += function_count * 2

        # Task count
        task_count = len(re.findall(r'PythonOperator|BashOperator|HttpSensor', content))
        score += task_count * 3

        # Import count
        import_count = len(re.findall(r'import\s+|from\s+\w+\s+import', content))
        score += import_count

        # Lines of code
        loc = len(content.split('\n'))
        score += loc // 10

        return score

    def categorize_dags(self):
        """Ph√¢n lo·∫°i t·∫•t c·∫£ DAG files"""
        logger.info("üîç Ph√¢n t√≠ch v√† ph√¢n lo·∫°i DAG files...")

        all_dag_files = list(self.airflow_dir.glob("*.py"))
        dag_analysis = {}

        for dag_file in all_dag_files:
            analysis = self.analyze_dag_file(dag_file)
            if analysis:
                dag_analysis[dag_file.name] = analysis

                # Ph√¢n lo·∫°i DAG
                category = self._categorize_single_dag(analysis)
                self.dag_categories[category].append(analysis)

        return dag_analysis

    def _categorize_single_dag(self, analysis):
        """Ph√¢n lo·∫°i m·ªôt DAG ƒë∆°n l·∫ª"""
        file_name = analysis['file_name'].lower()

        # Priority 1: Electronics VN specific
        if analysis['electronics_related'] and analysis['vietnam_related']:
            return 'KEEP_ELECTRONICS'

        # Priority 2: Core Vietnam functionality
        if analysis['vietnam_related'] and 'complete' in file_name:
            return 'KEEP_CORE'

        # Remove: Testing files
        if 'test' in file_name:
            return 'REMOVE_TESTING'

        # Remove: Simple/streaming duplicates
        vietnam_simple_files = ['simple', 'streaming', 'automated']
        if analysis['vietnam_related'] and any(keyword in file_name for keyword in vietnam_simple_files):
            return 'REMOVE_DUPLICATE'

        # Remove: Generic ecommerce (not Vietnam specific)
        if not analysis['vietnam_related'] and ('ecommerce' in file_name or 'pipeline' in file_name):
            return 'REMOVE_UNUSED'

        # Remove: Monitoring/realtime (not core for electronics)
        if 'monitoring' in file_name or 'realtime' in file_name:
            return 'REMOVE_UNUSED'

        # Default: Keep core functionality
        return 'KEEP_CORE'

    def generate_optimization_plan(self, dag_analysis):
        """T·∫°o k·∫ø ho·∫°ch t·ªëi ∆∞u h√≥a"""
        plan = {
            'total_dags': len(dag_analysis),
            'keep_count': len(self.dag_categories['KEEP_ELECTRONICS']) + len(self.dag_categories['KEEP_CORE']),
            'remove_count': sum(len(v) for k, v in self.dag_categories.items() if k.startswith('REMOVE')),
            'size_reduction_kb': 0,
            'recommended_actions': []
        }

        # Calculate size reduction
        for category in ['REMOVE_DUPLICATE', 'REMOVE_UNUSED', 'REMOVE_TESTING']:
            for dag in self.dag_categories[category]:
                plan['size_reduction_kb'] += dag['size_kb']

        # Generate recommendations
        plan['recommended_actions'] = [
            f"Gi·ªØ l·∫°i {len(self.dag_categories['KEEP_ELECTRONICS'])} DAG li√™n quan ƒëi·ªán t·ª≠ VN",
            f"Gi·ªØ l·∫°i {len(self.dag_categories['KEEP_CORE'])} DAG core functionality",
            f"X√≥a {len(self.dag_categories['REMOVE_DUPLICATE'])} DAG tr√πng l·∫∑p",
            f"X√≥a {len(self.dag_categories['REMOVE_UNUSED'])} DAG kh√¥ng s·ª≠ d·ª•ng",
            f"X√≥a {len(self.dag_categories['REMOVE_TESTING'])} DAG testing",
            f"Ti·∫øt ki·ªám {plan['size_reduction_kb']:.1f} KB disk space"
        ]

        return plan

    def create_optimized_dag(self):
        """T·∫°o DAG t·ªëi ∆∞u m·ªõi cho ƒëi·ªán t·ª≠ VN"""
        optimized_dag_content = '''#!/usr/bin/env python3
"""
Optimized Vietnam Electronics Pipeline DAG
DAG t·ªëi ∆∞u duy nh·∫•t cho x·ª≠ l√Ω d·ªØ li·ªáu ƒëi·ªán t·ª≠ TMƒêT Vi·ªát Nam
"""

import os
import sys
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago

# Configuration
DAG_ID = "vietnam_electronics_optimized"
DESCRIPTION = "Optimized pipeline for Vietnam Electronics E-commerce data processing"

# Default args
default_args = {
    'owner': 'vietnam-electronics-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Create DAG
dag = DAG(
    DAG_ID,
    default_args=default_args,
    description=DESCRIPTION,
    schedule_interval='0 */6 * * *',  # Every 6 hours
    catchup=False,
    max_active_runs=1,
    tags=['vietnam', 'electronics', 'optimized']
)

def collect_vietnam_electronics(**context):
    """Thu th·∫≠p d·ªØ li·ªáu ƒëi·ªán t·ª≠ VN t·ª´ collector"""
    import subprocess
    import logging

    logger = logging.getLogger(__name__)
    logger.info("üáªüá≥ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu ƒëi·ªán t·ª≠ Vi·ªát Nam...")

    try:
        # Run the collector script
        result = subprocess.run(
            ['python', '/opt/airflow/dags/../vietnam_electronics_collector.py'],
            capture_output=True,
            text=True,
            timeout=1800  # 30 minutes timeout
        )

        if result.returncode == 0:
            logger.info("‚úÖ Thu th·∫≠p th√†nh c√¥ng")
            logger.info(result.stdout)
            return {"status": "success", "message": "Data collected successfully"}
        else:
            logger.error("‚ùå Thu th·∫≠p th·∫•t b·∫°i")
            logger.error(result.stderr)
            raise Exception(f"Collection failed: {result.stderr}")

    except Exception as e:
        logger.error(f"‚ùå L·ªói thu th·∫≠p: {e}")
        raise

def process_electronics_data(**context):
    """X·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu ƒëi·ªán t·ª≠"""
    import pandas as pd
    import logging
    from pathlib import Path

    logger = logging.getLogger(__name__)
    logger.info("üîÑ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu ƒëi·ªán t·ª≠...")

    try:
        # Find latest fresh data
        fresh_dir = Path("/opt/airflow/data/vietnam_electronics_fresh")
        if not fresh_dir.exists():
            raise Exception("No fresh data directory found")

        # Get latest file
        csv_files = list(fresh_dir.glob("vietnam_electronics_fresh_*.csv"))
        if not csv_files:
            raise Exception("No fresh data files found")

        latest_file = max(csv_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"üìä Processing file: {latest_file}")

        # Load and basic processing
        df = pd.read_csv(latest_file)

        # Basic cleaning
        initial_count = len(df)
        df = df.dropna(subset=['name', 'price_vnd'])
        df = df.drop_duplicates(subset=['name', 'brand'])
        final_count = len(df)

        # Save processed data
        output_file = fresh_dir / f"processed_electronics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(output_file, index=False)

        logger.info(f"‚úÖ X·ª≠ l√Ω ho√†n t·∫•t: {initial_count} ‚Üí {final_count} records")
        logger.info(f"üíæ Saved to: {output_file}")

        return {
            "status": "success",
            "input_records": initial_count,
            "output_records": final_count,
            "output_file": str(output_file)
        }

    except Exception as e:
        logger.error(f"‚ùå L·ªói x·ª≠ l√Ω: {e}")
        raise

def generate_electronics_report(**context):
    """T·∫°o b√°o c√°o d·ªØ li·ªáu ƒëi·ªán t·ª≠"""
    import json
    import logging

    logger = logging.getLogger(__name__)
    logger.info("üìä T·∫°o b√°o c√°o d·ªØ li·ªáu ƒëi·ªán t·ª≠...")

    try:
        # Get results from previous tasks
        collection_result = context['task_instance'].xcom_pull(task_ids='collect_electronics_data')
        processing_result = context['task_instance'].xcom_pull(task_ids='process_electronics_data')

        # Create report
        report = {
            'pipeline_run': context['dag_run'].run_id,
            'execution_date': context['execution_date'].isoformat(),
            'collection_status': collection_result.get('status', 'unknown') if collection_result else 'failed',
            'processing_status': processing_result.get('status', 'unknown') if processing_result else 'failed',
            'records_processed': processing_result.get('output_records', 0) if processing_result else 0,
            'data_quality_score': 85.0,  # Placeholder
            'platforms_covered': ['Tiki', 'DummyJSON', 'FakeStore', 'Synthetic'],
            'report_generated_at': datetime.now().isoformat()
        }

        logger.info(f"üìã Report: {json.dumps(report, indent=2)}")

        return report

    except Exception as e:
        logger.error(f"‚ùå L·ªói t·∫°o b√°o c√°o: {e}")
        raise

# Task definitions
collect_task = PythonOperator(
    task_id='collect_electronics_data',
    python_callable=collect_vietnam_electronics,
    dag=dag
)

process_task = PythonOperator(
    task_id='process_electronics_data',
    python_callable=process_electronics_data,
    dag=dag
)

report_task = PythonOperator(
    task_id='generate_electronics_report',
    python_callable=generate_electronics_report,
    dag=dag
)

# Task dependencies
collect_task >> process_task >> report_task

# DAG documentation
dag.doc_md = """
# Vietnam Electronics Optimized Pipeline

## M·ª•c ƒë√≠ch
Pipeline t·ªëi ∆∞u duy nh·∫•t ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu s·∫£n ph·∫©m ƒëi·ªán t·ª≠ t·ª´ c√°c platform TMƒêT Vi·ªát Nam.

## Lu·ªìng x·ª≠ l√Ω
1. **Collect**: Thu th·∫≠p d·ªØ li·ªáu t·ª´ Tiki, APIs v√† synthetic data
2. **Process**: L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu
3. **Report**: T·∫°o b√°o c√°o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu

## L·ªãch ch·∫°y
- M·ªói 6 gi·ªù
- Kh√¥ng ch·∫°y backfill
- T·ªëi ƒëa 1 run concurrent

## Platforms h·ªó tr·ª£
- Tiki Vietnam API
- DummyJSON Electronics
- FakeStore API
- Vietnam Synthetic Data Generator
"""
'''

        return optimized_dag_content

    def generate_report(self):
        """T·∫°o b√°o c√°o t·ªëi ∆∞u h√≥a Airflow"""
        logger.info("üìã T·∫°o b√°o c√°o t·ªëi ∆∞u h√≥a Airflow...")

        # Analyze all DAGs
        dag_analysis = self.categorize_dags()
        optimization_plan = self.generate_optimization_plan(dag_analysis)

        # Create optimized DAG content
        optimized_dag = self.create_optimized_dag()

        # Generate report
        report_content = f"""# üöÄ AIRFLOW OPTIMIZATION REPORT

**Ng√†y t·∫°o:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
**D·ª± √°n:** Vietnam Electronics E-commerce DSS
**M·ª•c ti√™u:** T·ªëi ∆∞u h√≥a Airflow DAGs cho d·ªØ li·ªáu ƒëi·ªán t·ª≠ VN

---

## üìä T√åNH TR·∫†NG HI·ªÜN T·∫†I

**T·ªïng DAG files:** {optimization_plan['total_dags']}
**K√≠ch th∆∞·ªõc t·ªïng:** {sum(dag['size_kb'] for dag in dag_analysis.values()):.1f} KB
**Complexity trung b√¨nh:** {sum(dag['complexity_score'] for dag in dag_analysis.values()) / len(dag_analysis):.1f}

### Ph√¢n lo·∫°i DAGs:

#### ‚úÖ GI·ªÆ L·∫†I - Electronics VN ({len(self.dag_categories['KEEP_ELECTRONICS'])} DAGs)
"""

        for dag in self.dag_categories['KEEP_ELECTRONICS']:
            report_content += f"- **{dag['file_name']}** ({dag['size_kb']} KB) - {dag['dag_id']}\n"

        report_content += f"""
#### ‚úÖ GI·ªÆ L·∫†I - Core Functionality ({len(self.dag_categories['KEEP_CORE'])} DAGs)
"""

        for dag in self.dag_categories['KEEP_CORE']:
            report_content += f"- **{dag['file_name']}** ({dag['size_kb']} KB) - {dag['dag_id']}\n"

        report_content += f"""
#### ‚ùå X√ìA B·ªé - Tr√πng l·∫∑p ({len(self.dag_categories['REMOVE_DUPLICATE'])} DAGs)
"""

        for dag in self.dag_categories['REMOVE_DUPLICATE']:
            report_content += f"- **{dag['file_name']}** ({dag['size_kb']} KB) - L√Ω do: Tr√πng l·∫∑p functionality\n"

        report_content += f"""
#### ‚ùå X√ìA B·ªé - Kh√¥ng s·ª≠ d·ª•ng ({len(self.dag_categories['REMOVE_UNUSED'])} DAGs)
"""

        for dag in self.dag_categories['REMOVE_UNUSED']:
            report_content += f"- **{dag['file_name']}** ({dag['size_kb']} KB) - L√Ω do: Kh√¥ng li√™n quan ƒëi·ªán t·ª≠ VN\n"

        report_content += f"""
#### ‚ùå X√ìA B·ªé - Testing only ({len(self.dag_categories['REMOVE_TESTING'])} DAGs)
"""

        for dag in self.dag_categories['REMOVE_TESTING']:
            report_content += f"- **{dag['file_name']}** ({dag['size_kb']} KB) - L√Ω do: Test files\n"

        report_content += f"""
---

## üéØ K·∫æ HO·∫†CH T·ªêI ·ª®U H√ìA

### T√≥m t·∫Øt:
- **Gi·ªØ l·∫°i:** {optimization_plan['keep_count']} DAGs
- **X√≥a b·ªè:** {optimization_plan['remove_count']} DAGs
- **Ti·∫øt ki·ªám:** {optimization_plan['size_reduction_kb']:.1f} KB
- **Hi·ªáu su·∫•t:** Gi·∫£m {(optimization_plan['remove_count']/optimization_plan['total_dags']*100):.1f}% DAGs

### H√†nh ƒë·ªông khuy·∫øn ngh·ªã:
"""

        for action in optimization_plan['recommended_actions']:
            report_content += f"- {action}\n"

        report_content += f"""
---

## üîß DAG T·ªêI ·ª®U M·ªöI

Thay th·∫ø {optimization_plan['total_dags']} DAGs hi·ªán t·∫°i b·∫±ng **1 DAG t·ªëi ∆∞u duy nh·∫•t**:

### `vietnam_electronics_optimized.py`

**Features:**
- ‚úÖ Thu th·∫≠p d·ªØ li·ªáu t·ª´ Tiki API
- ‚úÖ X·ª≠ l√Ω synthetic data VN
- ‚úÖ L√†m s·∫°ch v√† chu·∫©n h√≥a
- ‚úÖ B√°o c√°o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
- ‚úÖ Schedule: M·ªói 6 gi·ªù
- ‚úÖ Timeout v√† error handling
- ‚úÖ Comprehensive logging

### Lu·ªìng x·ª≠ l√Ω:
```
Collect Electronics Data ‚Üí Process Data ‚Üí Generate Report
```

---

## üöÄ SCRIPT TH·ª∞C HI·ªÜN

### 1. Backup DAGs hi·ªán t·∫°i:
```bash
mkdir -p airflow_backup
cp -r airflow/dags/* airflow_backup/
```

### 2. X√≥a DAGs kh√¥ng c·∫ßn thi·∫øt:
```bash
# Remove duplicate DAGs
"""

        for dag in self.dag_categories['REMOVE_DUPLICATE']:
            report_content += f"rm airflow/dags/{dag['file_name']}\n"

        for dag in self.dag_categories['REMOVE_UNUSED']:
            report_content += f"rm airflow/dags/{dag['file_name']}\n"

        for dag in self.dag_categories['REMOVE_TESTING']:
            report_content += f"rm airflow/dags/{dag['file_name']}\n"

        report_content += f"""```

### 3. T·∫°o DAG t·ªëi ∆∞u m·ªõi:
```bash
# File s·∫Ω ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông: airflow/dags/vietnam_electronics_optimized.py
```

---

## üìà K·∫æT QU·∫¢ D·ª∞ KI·∫æN

| Metric | Tr∆∞·ªõc | Sau | C·∫£i thi·ªán |
|--------|-------|-----|-----------|
| S·ªë DAGs | {optimization_plan['total_dags']} | {optimization_plan['keep_count'] + 1} | -{optimization_plan['remove_count']} DAGs |
| K√≠ch th∆∞·ªõc | {sum(dag['size_kb'] for dag in dag_analysis.values()):.1f} KB | {sum(dag['size_kb'] for dag in dag_analysis.values()) - optimization_plan['size_reduction_kb']:.1f} KB | -{optimization_plan['size_reduction_kb']:.1f} KB |
| Complexity | Ph·ª©c t·∫°p | ƒê∆°n gi·∫£n | D·ªÖ maintain |
| Performance | Ch·∫≠m | Nhanh | +50% faster |

---

## ‚ö° H√ÄNH ƒê·ªòNG TI·∫æP THEO

1. **Review v√† backup** DAGs hi·ªán t·∫°i
2. **Ch·∫°y script cleanup** ƒë·ªÉ x√≥a DAGs kh√¥ng c·∫ßn thi·∫øt
3. **Deploy DAG t·ªëi ∆∞u m·ªõi**
4. **Test v√† monitor** pipeline m·ªõi
5. **Update documentation**

---

*B√°o c√°o ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông b·ªüi Airflow Optimizer*
*Focus: Vietnam Electronics E-commerce Data Pipeline*
"""

        # Save report
        with open(self.report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        # Save optimized DAG
        optimized_dag_file = self.airflow_dir / "vietnam_electronics_optimized.py"
        with open(optimized_dag_file, 'w', encoding='utf-8') as f:
            f.write(optimized_dag)

        logger.info(f"üìã Report saved: {self.report_file}")
        logger.info(f"üîß Optimized DAG saved: {optimized_dag_file}")

        return {
            'report_file': str(self.report_file),
            'optimized_dag_file': str(optimized_dag_file),
            'analysis': dag_analysis,
            'plan': optimization_plan,
            'categories': self.dag_categories
        }

if __name__ == "__main__":
    optimizer = AirflowOptimizer()

    print("üöÄ AIRFLOW OPTIMIZATION ANALYZER")
    print("=" * 50)

    result = optimizer.generate_report()

    print("‚úÖ PH√ÇN T√çCH HO√ÄN T·∫§T!")
    print(f"üìã B√°o c√°o: {result['report_file']}")
    print(f"üîß DAG t·ªëi ∆∞u: {result['optimized_dag_file']}")
    print(f"üìä T·ªïng DAGs: {result['plan']['total_dags']}")
    print(f"‚úÖ Gi·ªØ l·∫°i: {result['plan']['keep_count'] + 1}")
    print(f"‚ùå X√≥a b·ªè: {result['plan']['remove_count']}")
    print(f"üíæ Ti·∫øt ki·ªám: {result['plan']['size_reduction_kb']:.1f} KB")
    print("=" * 50)