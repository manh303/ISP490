#!/usr/bin/env python3
"""
Data Audit Report Generator
T·∫°o b√°o c√°o t·ªïng h·ª£p t√¨nh tr·∫°ng d·ªØ li·ªáu trong d·ª± √°n
"""

import os
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataAuditor:
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.data_dir = self.project_root / "data"
        self.backup_dir = self.project_root / "data_backup"
        self.report_file = self.project_root / f"DATA_AUDIT_REPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

    def check_data_warehouse(self):
        """Ki·ªÉm tra Data Warehouse"""
        logger.info("üè™ Ki·ªÉm tra Data Warehouse...")

        warehouse_info = {
            'status': 'EMPTY',
            'location': 'data_backup/warehouse/',
            'structure': {},
            'total_files': 0,
            'total_size_mb': 0
        }

        warehouse_path = self.backup_dir / "warehouse"
        if warehouse_path.exists():
            # Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c
            subdirs = [d.name for d in warehouse_path.iterdir() if d.is_dir()]
            warehouse_info['structure'] = {
                'subdirectories': subdirs,
                'details': {}
            }

            total_files = 0
            total_size = 0

            for subdir in subdirs:
                subdir_path = warehouse_path / subdir
                files = list(subdir_path.glob("*"))
                file_count = len(files)

                subdir_size = sum(f.stat().st_size for f in files if f.is_file())

                warehouse_info['structure']['details'][subdir] = {
                    'file_count': file_count,
                    'size_mb': round(subdir_size / (1024*1024), 2),
                    'files': [f.name for f in files if f.is_file()]
                }

                total_files += file_count
                total_size += subdir_size

            warehouse_info['total_files'] = total_files
            warehouse_info['total_size_mb'] = round(total_size / (1024*1024), 2)
            warehouse_info['status'] = 'EXISTS_BUT_EMPTY' if total_files == 0 else 'HAS_DATA'

        return warehouse_info

    def check_data_lake(self):
        """Ki·ªÉm tra Data Lake"""
        logger.info("üèûÔ∏è Ki·ªÉm tra Data Lake...")

        lake_info = {
            'status': 'EMPTY',
            'location': 'data_backup/lake/',
            'structure': {},
            'total_files': 0,
            'total_size_mb': 0
        }

        lake_path = self.backup_dir / "lake"
        if lake_path.exists():
            # Ki·ªÉm tra c·∫•u tr√∫c layers
            layers = [d.name for d in lake_path.iterdir() if d.is_dir()]
            lake_info['structure'] = {
                'layers': layers,
                'details': {}
            }

            total_files = 0
            total_size = 0

            for layer in layers:
                layer_path = lake_path / layer
                files = list(layer_path.glob("*"))
                file_count = len(files)

                layer_size = sum(f.stat().st_size for f in files if f.is_file())

                lake_info['structure']['details'][layer] = {
                    'file_count': file_count,
                    'size_mb': round(layer_size / (1024*1024), 2),
                    'files': [f.name for f in files if f.is_file()]
                }

                total_files += file_count
                total_size += layer_size

            lake_info['total_files'] = total_files
            lake_info['total_size_mb'] = round(total_size / (1024*1024), 2)
            lake_info['status'] = 'EXISTS_BUT_EMPTY' if total_files == 0 else 'HAS_DATA'

        return lake_info

    def check_current_data(self):
        """Ki·ªÉm tra d·ªØ li·ªáu hi·ªán t·∫°i"""
        logger.info("üìä Ki·ªÉm tra d·ªØ li·ªáu hi·ªán t·∫°i...")

        current_data = {
            'electronics_clean': {},
            'remaining_directories': {},
            'total_csv_files': 0,
            'total_data_size_mb': 0
        }

        # Ki·ªÉm tra vietnam_electronics_clean
        clean_path = self.data_dir / "vietnam_electronics_clean"
        if clean_path.exists():
            files = list(clean_path.glob("*.csv"))
            json_files = list(clean_path.glob("*.json"))

            # Ph√¢n t√≠ch file master
            master_files = [f for f in files if 'master' in f.name]
            master_info = {}

            if master_files:
                master_file = master_files[0]
                try:
                    df = pd.read_csv(master_file, nrows=1)  # ƒê·ªçc header
                    file_size = master_file.stat().st_size

                    # ƒê·∫øm t·ªïng s·ªë d√≤ng
                    with open(master_file, 'r', encoding='utf-8') as f:
                        total_lines = sum(1 for line in f) - 1  # Tr·ª´ header

                    master_info = {
                        'file_name': master_file.name,
                        'total_products': total_lines,
                        'columns': list(df.columns),
                        'column_count': len(df.columns),
                        'file_size_mb': round(file_size / (1024*1024), 2)
                    }
                except Exception as e:
                    master_info = {'error': str(e)}

            current_data['electronics_clean'] = {
                'status': 'EXISTS',
                'csv_files': len(files),
                'json_files': len(json_files),
                'master_file_info': master_info,
                'category_files': [f.name for f in files if 'master' not in f.name][:10]  # First 10
            }

        # Ki·ªÉm tra c√°c th∆∞ m·ª•c c√≤n l·∫°i
        remaining_dirs = ['electronics', 'consolidated', 'expanded_real', 'integrated', 'real_datasets', 'vietnamese_ecommerce']

        for dir_name in remaining_dirs:
            dir_path = self.data_dir / dir_name
            if dir_path.exists():
                csv_files = list(dir_path.glob("*.csv"))
                json_files = list(dir_path.glob("*.json"))

                dir_size = sum(f.stat().st_size for f in dir_path.glob("*") if f.is_file())

                current_data['remaining_directories'][dir_name] = {
                    'csv_files': len(csv_files),
                    'json_files': len(json_files),
                    'size_mb': round(dir_size / (1024*1024), 2),
                    'sample_files': [f.name for f in csv_files[:5]]  # First 5
                }

        # T·ªïng s·ªë file CSV
        total_csv = len(list(self.data_dir.rglob("*.csv")))

        # T·ªïng k√≠ch th∆∞·ªõc
        total_size = sum(f.stat().st_size for f in self.data_dir.rglob("*") if f.is_file())

        current_data['total_csv_files'] = total_csv
        current_data['total_data_size_mb'] = round(total_size / (1024*1024), 2)

        return current_data

    def check_collection_capability(self):
        """Ki·ªÉm tra kh·∫£ nƒÉng thu th·∫≠p d·ªØ li·ªáu"""
        logger.info("üîç Ki·ªÉm tra kh·∫£ nƒÉng thu th·∫≠p...")

        collection_info = {
            'collector_script': {
                'exists': False,
                'location': 'vietnam_electronics_collector.py',
                'features': []
            },
            'fresh_data_directory': {
                'exists': False,
                'location': 'data/vietnam_electronics_fresh/',
                'files': []
            },
            'dependencies': {
                'required_packages': ['requests', 'pandas', 'fake-useragent'],
                'status': 'unknown'
            }
        }

        # Ki·ªÉm tra collector script
        collector_path = self.project_root / "vietnam_electronics_collector.py"
        if collector_path.exists():
            collection_info['collector_script']['exists'] = True
            collection_info['collector_script']['features'] = [
                'Tiki API integration',
                'DummyJSON API',
                'FakeStore API',
                'Synthetic Vietnam data generation',
                'Multiple platform support'
            ]

        # Ki·ªÉm tra fresh data directory
        fresh_path = self.data_dir / "vietnam_electronics_fresh"
        if fresh_path.exists():
            collection_info['fresh_data_directory']['exists'] = True
            files = list(fresh_path.glob("*"))
            collection_info['fresh_data_directory']['files'] = [f.name for f in files]

        return collection_info

    def generate_recommendations(self, warehouse_info, lake_info, current_data, collection_info):
        """T·∫°o khuy·∫øn ngh·ªã"""
        recommendations = []

        # Data Warehouse recommendations
        if warehouse_info['status'] == 'EXISTS_BUT_EMPTY':
            recommendations.append({
                'priority': 'HIGH',
                'category': 'Data Warehouse',
                'issue': 'Data Warehouse r·ªóng',
                'recommendation': 'Thi·∫øt l·∫≠p pipeline ETL ƒë·ªÉ load d·ªØ li·ªáu ƒëi·ªán t·ª≠ VN v√†o warehouse',
                'action': 'T·∫°o schema v√† tables cho s·∫£n ph·∫©m ƒëi·ªán t·ª≠, sau ƒë√≥ load t·ª´ vietnam_electronics_clean'
            })

        # Data Lake recommendations
        if lake_info['status'] == 'EXISTS_BUT_EMPTY':
            recommendations.append({
                'priority': 'MEDIUM',
                'category': 'Data Lake',
                'issue': 'Data Lake r·ªóng',
                'recommendation': 'T·ªï ch·ª©c d·ªØ li·ªáu theo layers: raw ‚Üí processed ‚Üí analytics',
                'action': 'Di chuy·ªÉn d·ªØ li·ªáu ƒëi·ªán t·ª≠ v√†o lake structure v·ªõi proper partitioning'
            })

        # Current data recommendations
        if current_data.get('electronics_clean', {}).get('status') == 'EXISTS':
            master_info = current_data['electronics_clean'].get('master_file_info', {})
            if master_info.get('total_products', 0) < 1000:
                recommendations.append({
                    'priority': 'HIGH',
                    'category': 'Data Collection',
                    'issue': f'Ch·ªâ c√≥ {master_info.get("total_products", 0)} s·∫£n ph·∫©m ƒëi·ªán t·ª≠',
                    'recommendation': 'Thu th·∫≠p th√™m d·ªØ li·ªáu t·ª´ c√°c platform Vi·ªát Nam',
                    'action': 'Ch·∫°y vietnam_electronics_collector.py ƒë·ªÉ thu th·∫≠p th√™m d·ªØ li·ªáu'
                })

        # Collection capability recommendations
        if not collection_info['fresh_data_directory']['exists']:
            recommendations.append({
                'priority': 'MEDIUM',
                'category': 'Data Collection',
                'issue': 'Ch∆∞a c√≥ fresh data directory',
                'recommendation': 'Ch·∫°y collector ƒë·ªÉ t·∫°o d·ªØ li·ªáu m·ªõi',
                'action': 'python vietnam_electronics_collector.py'
            })

        return recommendations

    def generate_report(self):
        """T·∫°o b√°o c√°o t·ªïng h·ª£p"""
        logger.info("üìã T·∫°o b√°o c√°o t·ªïng h·ª£p...")

        # Thu th·∫≠p th√¥ng tin
        warehouse_info = self.check_data_warehouse()
        lake_info = self.check_data_lake()
        current_data = self.check_current_data()
        collection_info = self.check_collection_capability()
        recommendations = self.generate_recommendations(warehouse_info, lake_info, current_data, collection_info)

        # T·∫°o b√°o c√°o Markdown
        report_content = f"""# üìä B√ÅO C√ÅO KI·ªÇM TRA D·ªÆ LI·ªÜU D·ª∞ √ÅN

**Ng√†y t·∫°o:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
**D·ª± √°n:** Vietnam Electronics E-commerce DSS
**Ph·∫°m vi:** D·ªØ li·ªáu s·∫£n ph·∫©m ƒëi·ªán t·ª≠ TMƒêT Vi·ªát Nam

---

## üè™ DATA WAREHOUSE

**Tr·∫°ng th√°i:** `{warehouse_info['status']}`
**V·ªã tr√≠:** `{warehouse_info['location']}`
**T·ªïng files:** {warehouse_info['total_files']}
**K√≠ch th∆∞·ªõc:** {warehouse_info['total_size_mb']} MB

### C·∫•u tr√∫c:
"""

        # Data Warehouse structure
        if warehouse_info['structure'].get('subdirectories'):
            for subdir, details in warehouse_info['structure']['details'].items():
                report_content += f"- **{subdir}:** {details['file_count']} files ({details['size_mb']} MB)\n"
        else:
            report_content += "- Kh√¥ng c√≥ c·∫•u tr√∫c d·ªØ li·ªáu\n"

        report_content += f"""
---

## üèûÔ∏è DATA LAKE

**Tr·∫°ng th√°i:** `{lake_info['status']}`
**V·ªã tr√≠:** `{lake_info['location']}`
**T·ªïng files:** {lake_info['total_files']}
**K√≠ch th∆∞·ªõc:** {lake_info['total_size_mb']} MB

### Layers:
"""

        # Data Lake layers
        if lake_info['structure'].get('layers'):
            for layer, details in lake_info['structure']['details'].items():
                report_content += f"- **{layer}:** {details['file_count']} files ({details['size_mb']} MB)\n"
        else:
            report_content += "- Kh√¥ng c√≥ layer structure\n"

        report_content += f"""
---

## üì± D·ªÆ LI·ªÜU ƒêI·ªÜN T·ª¨ HI·ªÜN T·∫†I

### Vietnam Electronics Clean Data:
"""

        # Current electronics data
        if current_data.get('electronics_clean', {}).get('status') == 'EXISTS':
            electronics = current_data['electronics_clean']
            master = electronics.get('master_file_info', {})

            report_content += f"""
**Master File:** `{master.get('file_name', 'N/A')}`
**T·ªïng s·∫£n ph·∫©m:** {master.get('total_products', 0):,}
**S·ªë c·ªôt:** {master.get('column_count', 0)}
**K√≠ch th∆∞·ªõc:** {master.get('file_size_mb', 0)} MB
**Category files:** {electronics.get('csv_files', 0) - 1}

#### C√°c c·ªôt d·ªØ li·ªáu:
"""
            if master.get('columns'):
                for i, col in enumerate(master['columns'][:10], 1):  # First 10 columns
                    report_content += f"{i}. `{col}`\n"
                if len(master['columns']) > 10:
                    report_content += f"... v√† {len(master['columns']) - 10} c·ªôt kh√°c\n"
        else:
            report_content += "- Kh√¥ng c√≥ d·ªØ li·ªáu electronics clean\n"

        report_content += f"""
### Th∆∞ m·ª•c d·ªØ li·ªáu kh√°c:
"""

        # Other directories
        for dir_name, info in current_data.get('remaining_directories', {}).items():
            report_content += f"- **{dir_name}:** {info['csv_files']} CSV files ({info['size_mb']} MB)\n"

        report_content += f"""
**T·ªïng CSV files:** {current_data.get('total_csv_files', 0)}
**T·ªïng k√≠ch th∆∞·ªõc d·ªØ li·ªáu:** {current_data.get('total_data_size_mb', 0)} MB

---

## üîÑ KH·∫¢ NƒÇNG THU TH·∫¨P D·ªÆ LI·ªÜU

### Collector Script:
**Tr·∫°ng th√°i:** {'‚úÖ C√≥ s·∫µn' if collection_info['collector_script']['exists'] else '‚ùå Ch∆∞a c√≥'}
**File:** `{collection_info['collector_script']['location']}`

#### T√≠nh nƒÉng:
"""

        for feature in collection_info['collector_script']['features']:
            report_content += f"- {feature}\n"

        report_content += f"""
### Fresh Data Directory:
**Tr·∫°ng th√°i:** {'‚úÖ ƒê√£ t·∫°o' if collection_info['fresh_data_directory']['exists'] else '‚ùå Ch∆∞a t·∫°o'}
**V·ªã tr√≠:** `{collection_info['fresh_data_directory']['location']}`
**Files:** {len(collection_info['fresh_data_directory']['files'])}

---

## üéØ KHUY·∫æN NGH·ªä

"""

        # Recommendations
        for i, rec in enumerate(recommendations, 1):
            priority_emoji = {'HIGH': 'üî¥', 'MEDIUM': 'üü°', 'LOW': 'üü¢'}.get(rec['priority'], '‚ö™')
            report_content += f"""
### {i}. {priority_emoji} {rec['category']} - {rec['priority']} Priority

**V·∫•n ƒë·ªÅ:** {rec['issue']}
**Khuy·∫øn ngh·ªã:** {rec['recommendation']}
**H√†nh ƒë·ªông:** `{rec['action']}`

"""

        report_content += f"""---

## üìà T√ìM T·∫ÆT T·ªîNG QUAN

| Th√†nh ph·∫ßn | Tr·∫°ng th√°i | K√≠ch th∆∞·ªõc | Ghi ch√∫ |
|------------|------------|------------|---------|
| Data Warehouse | {warehouse_info['status']} | {warehouse_info['total_size_mb']} MB | {warehouse_info['total_files']} files |
| Data Lake | {lake_info['status']} | {lake_info['total_size_mb']} MB | {lake_info['total_files']} files |
| Electronics Data | {'‚úÖ ACTIVE' if current_data.get('electronics_clean', {}).get('status') == 'EXISTS' else '‚ùå MISSING'} | {current_data.get('total_data_size_mb', 0)} MB | {current_data.get('total_csv_files', 0)} CSV files |
| Collector | {'‚úÖ READY' if collection_info['collector_script']['exists'] else '‚ùå MISSING'} | - | Vietnam platforms support |

---

## üöÄ H√ÄNH ƒê·ªòNG TI·∫æP THEO

1. **N·∫øu c·∫ßn thu th·∫≠p d·ªØ li·ªáu m·ªõi:**
   ```bash
   pip install requests pandas fake-useragent
   python vietnam_electronics_collector.py
   ```

2. **N·∫øu c·∫ßn setup Data Warehouse:**
   - T·∫°o schema PostgreSQL cho electronics
   - Load d·ªØ li·ªáu t·ª´ vietnam_electronics_clean
   - Setup ETL pipeline

3. **N·∫øu c·∫ßn setup Data Lake:**
   - T·ªï ch·ª©c d·ªØ li·ªáu theo layers (raw/processed/analytics)
   - Implement data partitioning
   - Setup metadata catalog

---

*B√°o c√°o ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông b·ªüi Data Auditor*
*D·ª± √°n: Vietnam Electronics E-commerce DSS*
"""

        # L∆∞u b√°o c√°o
        with open(self.report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        logger.info(f"üìã B√°o c√°o ƒë√£ l∆∞u: {self.report_file}")

        return {
            'report_file': str(self.report_file),
            'warehouse_info': warehouse_info,
            'lake_info': lake_info,
            'current_data': current_data,
            'collection_info': collection_info,
            'recommendations': recommendations
        }

if __name__ == "__main__":
    auditor = DataAuditor()

    print("üîç VIETNAM ELECTRONICS DATA AUDIT")
    print("=" * 50)

    result = auditor.generate_report()

    print("‚úÖ AUDIT HO√ÄN T·∫§T!")
    print(f"üìã B√°o c√°o: {result['report_file']}")
    print(f"üìä Data Warehouse: {result['warehouse_info']['status']}")
    print(f"üèûÔ∏è Data Lake: {result['lake_info']['status']}")
    print(f"üì± Electronics Data: {result['current_data'].get('electronics_clean', {}).get('status', 'MISSING')}")
    print(f"üéØ Khuy·∫øn ngh·ªã: {len(result['recommendations'])} items")
    print("=" * 50)